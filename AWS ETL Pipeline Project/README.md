# ETL Pipeline Project
### Overview
This project is an exact replica of a commercial ETL pipeline. We were given data which had to be extracted from an external database, transformed according to the ERDs and star schema we had been given and then loaded to a data lakehouse at the end for more permanent storage to be used by the "client". This project was completed over the course of two weeks by me and four other team members.

### Project Details
Data was supplied to us in a 3rd party database in the form of a series of SQL tables. These tables were updated every 30 minutes with all of the data relating to the sales made by a fictional company.

In the first stage we extracted this data from the database, converted it from SQL to CSV file format for more efficient parsing later down the line and then uploaded it to the AWS S3 "ingested" bucket we had created. This was automated and done via a script with a new ingestion occurring every time the raw source data was updated. It was run and logged by AWS Lambda and CloudWatch with success/error messages sent out by email to us as appropriate with every run. 

We split the second stage of the process into three parts - the payment, purchase and sales data. Each set of data was handled individually and had to be processed on its own. For each, the pertinent data was extracted from the bucket, read and each CSV file converted into a dataframe. The dataframes then had to be queried using pandasql and SQLite and thus transformed into the fact and dim tables stipulated in the project brief. The final step in this stage was to turn them into parquet format, so they could be stored efficiently, long-term at a fraction of the memory cost, and upload them to a secondary AWS S3 bucket for the "processed" data. Here, each iteration of the data is housed for every update to the original tables. Again, this was carried out by a script and automated using AWS Lambdas.

Our final stage involved transferring all of this processed, parquet data from the S3 bucket to a data lakehouse to be stored long-term. This way it is readily available to be used by the "client" whenever necessary, takes up minimal space and is available long into the future to be included in studies for years to come. Also automated and scripted.

The entire project was tested thoroughly as we went along using TDD methods, ensuring all tests passed, all functions performed their exact function and this could be replicated and that this could be proven explicitly using test data.

### Additional Info
The bit I was in charge of within the project was processing the payments data. This involved researching and planning end-to-end how we go from the input data to the output data. Each step had to be researched and the most efficient, appropriate, sometimes testable or even the only way to do it had to be found and used. After this was all put together in a contiguous set of functions it had to be tested meticulously, using TDD methodology throughout, to ensure it worked unequivocally and that any future engineer coming to observe this code could be sure of this too. Each function was tested using mocking and patching and fake test data with the use of pytest ensuring each function worked for all scenarios it would encounter. All tests pass.

The bits I was most proud of within my section were the automated feature I programmed in so that my code would check for how many sets of processed data were already present and then only run code on data more recent than that, thus ensuring legacy data was not added to the stack and processing power was not wasted on unnecessary tasks, and learning SQLite on my own in order to be able to write the pandasql queries for the dataframes to out put the correct data, as this was the only language they accepted and I had not come across it before.

<!--
change 1st and 2nd sentence in additional info to make it more formal and give more detail about what the ends of the "end-to-end" were - see below.

 - 1NF, 2NF, 3NF thing (after dim tables in 4th paragraph, 3rd row)

/mock/pseudo-data - last paragraph

The bit I was in charge of within the project was processing the payments data.
My main duty within the project was to be in charge of processing the payments data. This involved researching and coming up with an end-to-end solution of how we go from a series of CSV files in an S3 bucket to the requested, formatted and ordered tables in parquet format in separate another. ll through scripted Python code - and then implementing this code/solutuion so it worked in real life reali life appications/practical applications. It also involved testing it exhaustively using TDD mothod/ology to ensure that no function would fail under any circumstances / in any event/ scenario.

The parts of the project I am particularly proud of are adding in an extra feature so that the code would check for how many processed files there were and then only run the code on new fikes that had been received since this last/latetst processing, so that no processing power were wasted and no new old files were added to pre-existing/legacy/archaic dtaa. The second part is that I self-taught myself SQLite in order to run the SQL queries on the dataframes in pandasql as it was the only language it would accept and therefore i had no choice bu ttod o so in order to cpklete the project.

I was responsible, within the project, for processing all of the payment data.
My main responsibility within the project was being in charge of processing all of the payment data. 
My main responsibility within the project was to process all of the payment data. This meant devising an end-to-end step-by-step strategy of how we could go from CSV file data in one bucket to formatted parquet data in another. I had to research all of the stages required within this, the different methods with which each could be carried out, the quickest/simplest, most suitable, which fit in best with our code or most easily testable way and then decide hci of these to go for, and then write the code, SQL, check it went together back to back which was the most efficient, most suitable, fut in best with what we were trying to achieve and the rest of our code, the most easily testable and then sdevise how these would fit together in one flowing , researching   researching methods, coming up with a step by step plan from beginning to end of how the entire process could be carried out successfully from start to finish, writing all of the code, SQL, checking all of the functions working in tandem with each other 
I ws particularly proud of the fact my code had a special safety measure in place so it would check how many processed data sets there were and only 

into the (format) and then (SQL-queried) to produce the right output tables as requested in the project brief. [Finally the output data had to be converted (in)to parquet (format) for more (concise and) data-efficient long-term storage and then loaded to the processed S3 bukcet in a directory designating its run number and under a fielname detailing the data/table it represnted. uploaded to the S3 bucket designated for processed data under a directory and filename detailing its run number and table name/table it represnted. (categorsied?/logged?] Again, this entire stage was automated and scripted and therefore run from start to finish all on its own,( without intereference,) including checking for and ensuring only new data was run through the program each time and added to the processed bucket. The final stage was to move this processed data along from the secondary AWS S3 bucket to the datalake for permanant, long-term storage to be utilised/pulled by the "client" whenever they feel it necessary.

data and coluns in thr output data as requested put it into the format/supply the data/columns requested by the "client". Following this it was converted to parquet and then saved under a filename specifying its run number and the tablename in a secondary bucket AWS S3 bucket - processed. Next, it had to be  to pull this data from the ingestion bucket, and then store it in an AWS S3 bucket. During this process we converted it from SQL to CSV (file) format for more swifter parsing later down the line. and then (up)load it for initial storage to an AWS S3 bucket we had created. During this process we converted it from SQL to CSV (file) format for more time-efficient parsing/swifter data manipulation later down the line. We were taksed initially with extracting this data, converting it from SQL to CSV format for swifter manipulation and then uploading it to an AWS S3 bucket we had created for primary storage. This was all done via a script and tracked and logged using AWS Lambda and CloudWatch, with success/error alerts for each iteration sent to us/out via email. Once in the intitial (ingestion) bucket, the rlevant data had to be extracted, read and then compiled into dataframe format to be transformed using SQL queries into the client-requested data specified by the ERDs. For this, for the method I used, I had to teach myself SQLite as this was the only language pandasql qould accept queries in in order to format dataframes. The final step in this part of the process was converting these () into parquet format for concise and data-efficient storage and then saving them under unique filenames to a secondary (processed) S3 bucket for the final data. I had to  ed data required format requested by the client it had to be transofrmed a  Our task was to extract this data, store it in an AWS S3 bucket, transform it into the format stipulated by the ERD diagrams and then load it to a data lakehouse for storing (in a form useful to the end-client).

The process for this was all scripted and all processes from downloading to extracting to transforming to loading were tracked and logged by AWS Lambda and CloudWatch. The process for We then had to extract this data and save it in CSV format to an AWS S3 bucket we had created - let's call this the ingestion bucket. (This was all done via a script including the creation of the S3 bucket). From here, the relevant data had to be identified, extracted, read, (converted to dataframes and then) transformed using SQL into the tables on the ERD diagrams we were given and then uploaded in parquet format to a second S3 bucket (we had created) to store the data relevant to the client - the processed bucket. The final stage was to move this data on to a data lakehouse for final storage in an external database.

tracked and logged using AWS Lambda and CloudWatch was completed over the course of two weeks by me and 4 other team members. Outside of the project brief we were given no guidance and so had to use the skills we had accumulated on the course in order to strategise and complete it. This included coming up with a flow chart of work necessary to complete it from start to finish, producing a kanban board on Trello to document and teack progress, assign tasks equally between us and rotate leading team meetings each morning in order to keep up to date on progress and make sure the project as a whole was on track for our deadline date.
<br>

## The Project Itself
In the project itself we were given the task of creating an ETL pipeline. The pipeline would extract data from an online database in the form of SQL table and store it as CSV files in an ingestion bucket (S3) on AWS. The task was then to It was then the task of a few of us to pull this data from the bucket, turn it into dataframes, transform ot based on the ERD diagrams (shcema?) we werre given in the rpoject brief using SQL queries, convert it to parquet format and then load it to a processed AWS S3 bucket for compact storage. The task was then to pull the data from the bucket, transform it based on the RED diasgrams given and then load it in parquet format to the processed AWS bucket. This was all done in/via a script. FInally it was passed on to a final database for storage of the most recent form of the data. Every half hour, new data would come in and the process would have to ben run again, updating th data in the final database.

Everything was logged using lambdas and AWS CloudWatch, with appropriate error messages where necessary. The pulling, transforming and re-uploading of the data has error-handling and all of the features within the project have been tested using TDD (pytest with mocking and patching).

My main duty was the overseeing of the transforming of the payment data, this included all of the data relating to payments, processing based on the schema and producing tabnles from the raw data which reflected the desried data output. There are additional features added to my code to ensure only new data is run by the code and transformed and SQL queries were written in SQLite which was self-taught as this was he only language that pandasql accepted and was not taught/included ad part of the course.

The project was a great learning experieice and a great insight into what the life of a data engineer involves and what the day-to-day is like.
-->
